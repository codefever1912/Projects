# -*- coding: utf-8 -*-
"""Sentiment-Analyzer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dJ952kuIeA7GxV_2_cVEkGEnBn1-0gHN
"""

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
import nltk
import re
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.feature_extraction.text import TfidfVectorizer
from gensim.models import Word2Vec
from nltk.tokenize import word_tokenize
import matplotlib.pyplot as plt
nltk.download("punkt_tab")

data = pd.read_csv("/content/twitter_data.csv")
data = data.dropna()
data['category'] = data['category'].astype(int)

def preprocess(tweet):
    assert type(tweet)==str, "Invalid type provided"

    # tweet = tweet.lower()
    tweet = re.sub(r'[^a-zA-Z\s]', '', tweet) #removes non-alphabet characters
    tweet = re.sub(r'http\S+|www|S+|https\S+', '', tweet, flags=re.MULTILINE) #removes links and URLs
    tweet = re.sub(r'@\w+', '', tweet) #removes @ mentions
    tweet = re.sub(r'#', '', tweet)#removes hashtags

    tokenized_tweet = word_tokenize(tweet) #Tokenizes the tweet, separating tweet into individual tokens and joining them to form a single long string of characters

    return tokenized_tweet

tweets = data['clean_text']
tokenized_tweets = [preprocess(tweet) for tweet in tweets]

# w2v_model = Word2Vec(vector_size=100,min_count=3,window=5,sg=1,workers=4)
# w2v_model.build_vocab(tokenized_tweets)

# w2v_model.train(
#     tokenized_tweets,
#     total_examples = w2v_model.corpus_count,
#     epochs=20,
#     compute_loss = True,
# )

# Introducing TF-IDF instead of w2v for vectorization
vectorizer = TfidfVectorizer(max_features=500)
X = torch.tensor(vectorizer.fit_transform(tweets).toarray())
y = torch.tensor(data['category'].values)

# def tweet_to_vector(tweet, model=w2v_model, vector_size=100):
#     tweet_vec = np.zeros(vector_size)
#     count = 0
#     for word in tweet:
#         if word in w2v_model.wv:
#             tweet_vec += w2v_model.wv[word]
#             count += 1
#     if count != 0:
#         tweet_vec /= count
#     return tweet_vec

# X = torch.FloatTensor([np.array(tweet_to_vector(tweet)) for tweet in tokenized_tweets])
# y = torch.LongTensor(data['category'])

#Creating Pytorch datasets(and dataloaders)
class SentimentDataset(Dataset):
    def __init__(self,x,y):
        self.features = x #Represents the vectorized tweets by the tokenizer(Word2Vec/TF-IDF)
        self.labels = y - y.min()

    def __len__(self):
        return len(self.features)

    def __getitem__(self,idx):
        return torch.as_tensor(self.features[idx], dtype=torch.float32), torch.as_tensor(self.labels[idx], dtype=torch.int64)

x_train, x_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42)

train_data = SentimentDataset(x_train, y_train)
test_data = SentimentDataset(x_test, y_test)

BATCH_SIZE = 32
train_dataloader = DataLoader(train_data,batch_size=BATCH_SIZE,shuffle=True,drop_last=True)
test_dataloader = DataLoader(test_data,batch_size=BATCH_SIZE,shuffle=True,drop_last=True)

class SA(nn.Module):
    def __init__(self,input_size,hidden_units,output_size):
        super().__init__()
        self.analyzer = nn.Sequential(
            nn.Linear(input_size, hidden_units),
            nn.LeakyReLU(),
            nn.Dropout(0.4),
            nn.Linear(hidden_units, output_size),
        )

    def forward(self, x):
        return self.analyzer(x)

"""12.11

*   Upon running the model, gives an error saying that the target is out of bounds.
This is because CELoss expects the targets to start from 0 to num_classes - 1. Solution for this is to create a mapping, as simple as adding 1 to the target to make them start from 1 or currently by min-max normalization

* So since TF-IDF vectorizer returns a sparse array that makes it easy to store high dimensonal data, we need to convery it to an array in order to be able to convert them to PyTorch datasets



"""

INPUT_SIZE = X.shape[1]
HIDDEN_UNITS = 64
OUTPUT_SIZE = 3

device = "cuda" if torch.cuda.is_available() else "cpu"
model = SA(INPUT_SIZE, HIDDEN_UNITS, OUTPUT_SIZE).to(device)

loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)
epochs = 5

for epoch in range(epochs):
    train_loss, train_acc = 0,0
    for (x_train, y_train) in train_dataloader:
        x_train, y_train = x_train.to(device), y_train.to(device)

        train_logits = model(x_train)
        train_preds = train_logits.argmax(dim=1)
        loss = loss_fn(train_logits, y_train).to(device)
        train_loss += loss.item()
        acc = accuracy_score(y_train.cpu(), train_preds.cpu())
        train_acc += acc
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    train_loss /= len(train_dataloader)
    train_acc /= len(train_dataloader)
    print(f"Epoch : {epoch+1} / {epochs} | Train Loss : {train_loss:.3f} | Train accuracy : {train_acc:.2f}%")

test_loss, test_acc = 0,0
for (x_test, y_test) in test_dataloader:
    x_test, y_test = x_test.to(device), y_test.to(device)
    test_logits = model(x_test)
    test_preds = test_logits.argmax(dim=1)
    loss = loss_fn(test_logits, y_test)
    test_loss += loss.item()
    acc = accuracy_score(y_test.cpu(), test_preds.cpu())
    test_acc += acc
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

test_loss /= len(test_dataloader)
test_acc /= len(test_dataloader)
print(f"Test Loss : {test_loss:.3f} | Test accuracy : {test_acc:.2f}%")

#Seeing the imbalance of dataset
a,b,c = 0,0,0
for x in y:
    if x == -1:
        a += 1
    elif x == 0:
        b += 1
    else:
        c += 1

print(f"-1 : {a}/{len(y)} | 0 : {b}/{len(y)} | 1 : {c}/{len(y)}")
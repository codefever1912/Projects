# -*- coding: utf-8 -*-
"""Sentiment140.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Dyb2fkthJVetj4eCWMOBPMFFimqcOIdo
"""

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
import numpy as np
import pandas as pd
import re
from collections import Counter
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import sklearn
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import confusion_matrix, accuracy_score, f1_score,recall_score

nltk.download("punkt_tab")
nltk.download("stopwords")
stop_words = set(stopwords.words('english'))

def preprocess(tweet):
    """Enhanced preprocessing function"""
    assert isinstance(tweet, str), "Invalid type provided"

    # Convert to lowercase
    tweet = tweet.lower()

    # Remove URLs, mentions, hashtags, and non-alphabetic characters
    tweet = re.sub(r'http\S+|www\S+|https\S+', '', tweet, flags=re.MULTILINE)
    tweet = re.sub(r'@\w+', '', tweet)
    tweet = re.sub(r'#', '', tweet)
    tweet = re.sub(r'[^a-zA-Z\s]', '', tweet)

    # Tokenize
    tokens = word_tokenize(tweet)

    # Remove stopwords and short words
    tokens = [token for token in tokens if token not in stop_words and len(token) > 2]

    return ' '.join(tokens)

class SA(nn.Module):
    def __init__(self,in_features, hidden_features, out_features):
        super().__init__()
        self.analyzer = nn.Sequential(
            nn.Linear(in_features, hidden_features),
            nn.BatchNorm1d(hidden_features),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(hidden_features,hidden_features//2),
            nn.BatchNorm1d(hidden_features//2),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(hidden_features//2, out_features)
        )

    def forward(self,x):
        return self.analyzer(x)

class SentimentDataset(Dataset):
    def __init__(self,x,y,scaler=None):
        if scaler is not None:
            self.features = torch.FloatTensor(scaler.transform(x))
        else:
            self.features = torch.FloatTensor(x)

        y = y - y.min() #Min-max normalization, making all the values > 0
        self.labels = torch.LongTensor(y)

    def __len__(self):
        return len(self.features)

    def __getitem__(self, idx):
        return self.features[idx], self.labels[idx]

def train_model(
        model : torch.nn.Module,
        train_dataloader : torch.utils.data,
        test_dataloader : torch.utils.data,
        loss_fn : torch.nn,
        optimizer : torch.optim,
        accuracy_fn : sklearn.metrics,
        epochs : int,
        device : str,
        scheduler : torch.optim = None,
):

    model = model.to(device)
    for epoch in range(epochs):
        model.train()
        train_loss, train_acc = 0,0
        for (train_X, train_y) in train_dataloader:
            train_X, train_y = train_X.to(device), train_y.to(device)

            optimizer.zero_grad()
            train_logits = model(train_X).to(device)
            loss = loss_fn(train_logits, train_y)

            loss.backward()
            optimizer.step()

            with torch.no_grad():
                train_preds = train_logits.argmax(dim=1)
                train_loss += loss.item()
                train_acc += accuracy_fn(train_y.cpu(), train_preds.cpu())

        train_loss /= len(train_dataloader)
        train_acc /= len(train_dataloader)

        #Testing the model
        model.eval()
        test_loss, test_acc = 0,0
        with torch.no_grad():
            for (test_X, test_y) in test_dataloader:
                test_X, test_y = test_X.to(device), test_y.to(device)
                test_logits = model(test_X).to(device)
                test_preds = test_logits.argmax(dim=1)
                test_loss += loss_fn(test_logits, test_y).item()
                test_acc += accuracy_fn(test_y.cpu(), test_preds.cpu())

        test_loss /= len(test_dataloader)
        test_acc /= len(test_dataloader)

        if scheduler is not None:
            scheduler.step(test_loss)


        print(f"Epoch : {epoch+1} / {epochs}")
        print(f"Train Loss : {train_loss:.3f} | Train accuracy : {train_acc*100:.2f}%")
        print(f"Test Loss : {test_loss:.3f} | Test accuracy : {test_acc*100:.2f}%")
        print("-"*20)

def val_model(
        model : torch.nn.Module,
        val_dataloader : torch.utils.data,
        loss_fn : torch.nn,
        accuracy_fn : sklearn.metrics,
        device : str,
):

    model = model.to(device)
    model.eval()
    val_loss, val_acc = 0,0
    with torch.no_grad():
        for (val_x, val_y) in val_dataloader:
            val_x, val_y = val_x.to(device), val_y.to(device)
            val_logits = model(val_x).to(device)
            val_preds = val_logits.argmax(dim=1)
            val_loss += loss_fn(val_logits, val_y).item()
            val_acc += accuracy_fn(val_y.cpu(), val_preds.cpu())

    val_acc /= len(val_dataloader)
    val_loss /= len(val_dataloader)

    print(f"Val loss : {val_loss:.3f} | Val accuracy : {val_acc*100:.2f}%")
    print("-"*20)

data = pd.read_csv("./Datasets/sentiment140.csv", names=["target", "id", "data", "flag", "user", "tweet"],nrows=1000000)
# data = pd.read_csv("/content/twitter_data.csv")
# data_list = []
# for chunk in data:
#     chunk = chunk[['target', 'tweet']]
#     chunk["target"] = chunk["target"].astype(int)
#     data_list.append(chunk)
# # data = pd.concat(data_list)
data = data[['target', 'tweet']] #Keeping only the relevant columns
data = data.dropna()
# data['category'] = data['category'].astype(int)

data['tweet'] = data['tweet'].apply(preprocess)
labels = data['target']

#Vectorizer
vectorizer = TfidfVectorizer(max_features=750,ngram_range=(1,2))
X = vectorizer.fit_transform(data['tweet']).toarray() #Converting to array as other process cannot process sparse matrices, the default return type of TF-IDF fit_transform()

#Scaling the data
scaler = StandardScaler()
X = scaler.fit_transform(X)

#Applying label encoding for the targets
le = LabelEncoder()
y = le.fit_transform(labels)

# Splitting data and creating datasets
train_X, temp_X, train_y, temp_y = train_test_split(X,y,test_size=0.3,random_state=42,stratify=y)
test_X, val_X, test_y, val_y = train_test_split(temp_X,temp_y,test_size=0.5,random_state=42,stratify=temp_y)

# train_X, test_X, train_y, test_y = train_test_split(X,y,test_size=0.15,random_state=42,stratify=y)

train_dataset = SentimentDataset(train_X, train_y)
test_dataset = SentimentDataset(test_X, test_y)
val_dataset = SentimentDataset(val_X, val_y)

#Dataloaders
BATCH_SIZE = 32
train_dataloader = DataLoader(train_dataset,batch_size=BATCH_SIZE,shuffle=True)
test_dataloader = DataLoader(test_dataset,batch_size=BATCH_SIZE)
val_dataloader = DataLoader(val_dataset,batch_size=BATCH_SIZE)

#Training and Validating
device = "cuda" if torch.cuda.is_available() else "cpu"
model = SA(
    in_features=X.shape[1],
    hidden_features=128,
    out_features=3,
)

class_counts = Counter(labels)
max_samples = max(class_counts.values())
class_weights = torch.FloatTensor([max_samples / count for count in class_counts.values()])

loss_fn = nn.CrossEntropyLoss(weight=class_weights.to(device))
optimizer = torch.optim.Adam(params = model.parameters(), lr=3e-4)
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, mode='min', factor=0.1, patience=3,
)

train_model(model, train_dataloader, test_dataloader, loss_fn, optimizer, accuracy_score,10,device,scheduler)
val_model(model,val_dataloader,loss_fn,accuracy_score, device)